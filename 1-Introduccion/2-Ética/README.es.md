# IntroducciÃ³n a la Ã©tica de los datos

|![Alt text](./img/image.png)|
|:---:|
| Ã‰tica de la ciencia de datos: nota de boceto de [@nitya](https://twitter.com/nitya)_ |

---

We are all data citizens living in a datafied world.

Todos somos ciudadanos de datos que vivimos en un mundo con datos.

Las tendencias del mercado nos dicen que en el aÃ±o 2022, 1 de cada 3 organizaciones grandes compro y vendio sus datos a travÃ©s de [mercados e intercambios](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) en lÃ­nea. Como **desarrolladores de aplicaciones**, nos resultarÃ¡ mÃ¡s fÃ¡cil y econÃ³mico integrar conocimientos basados â€‹â€‹en datos y automatizaciÃ³n basada en algoritmos en las experiencias diarias de los usuarios. Pero a medida que la IA se vuelve omnipresente, tambiÃ©n necesitaremos comprender los daÃ±os potenciales causados â€‹â€‹por la [utilizaciÃ³n](https://www.youtube.com/watch?v=TQHs8SA1qpk) de tales algoritmos como armas a escala.

Las tendencias tambiÃ©n indican que crearemos y consumiremos mÃ¡s de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de datos para 2025. Como **cientÃ­ficos de datos**,esto nos brinda niveles de acceso a datos personales sin precedentes. Esto significa que podemos crear perfiles de comportamiento de los usuarios e influir en la toma de decisiones de manera que creen una [ilusiÃ³n de libre elecciÃ³n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) y al mismo tiempo, impulsen potencialmente a los usuarios hacia los resultados que preferimos. TambiÃ©n plantea preguntas mÃ¡s amplias sobre la privacidad de los datos y la protecciÃ³n de los usuarios.

La Ã©tica de los datos es ahora una barrera necesaria para la ciencia y la ingenierÃ­a de datos, y nos ayuda a minimizar los daÃ±os potenciales y las consecuencias no deseadas de nuestras acciones basadas en datos. El [Gartner Hype Cycle for AI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) dentifica tendencias relevantes en Ã©tica digital, IA responsable y gobernanza de la IA como impulsores clave de megatendencias mÃ¡s amplias en torno a la _democratizaciÃ³n_ e _industrializaciÃ³n_ de la IA.

![Gartner's Hype Cycle for AI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

En esta lecciÃ³n, exploraremos el fascinante Ã¡rea de la Ã©tica de los datos, desde conceptos y desafÃ­os centrales hasta estudios de casos y conceptos de IA aplicados como la gobernanza, que ayudan a establecer una cultura Ã©tica en equipos y organizaciones que trabajan con datos e IA.


## [Pre-lecture quiz](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/2) ðŸŽ¯

## Definiciones basicas

Comencemos por comprender la terminologÃ­a bÃ¡sica.

La palabra â€œÃ©ticaâ€ proviene del [vocablo griego â€œethikosâ€](https://en.wikipedia.org/wiki/Ethics) (y su raÃ­z â€œethosâ€) que significa carÃ¡cter o naturaleza moral .

**La Ã©tica** se trata de los valores compartidos y los principios morales que gobiernan nuestro comportamiento en la sociedad. La Ã©tica no se basa en leyes sino en normas ampliamente aceptadas sobre lo que es "lo correcto y lo incorrecto". Sin embargo, las consideraciones Ã©ticas pueden influir en las iniciativas de gobierno corporativo y las regulaciones gubernamentales que crean mÃ¡s incentivos para el cumplimiento.

**La Ã‰tica de Datos** es una [nueva rama de la Ã©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estudia y evalÃºa problemas morales relacionados con  _datos, algoritmos y prÃ¡cticas correspondientes_". AquÃ­, **"los datos"**  se centran en acciones relacionadas con la generaciÃ³n, el registro, la conservaciÃ³n, el procesamiento, la difusiÃ³n, el intercambio y el uso de los **"algoritmos"** se centran en la IA, los agentes, el aprendizaje automÃ¡tico y los robots, y las **"prÃ¡cticas"**  se centran en temas como la innovaciÃ³n responsable. programaciÃ³n, hacking y cÃ³digos de Ã©tica.

**La Ã‰tica Aplicada** es la [aplicaciÃ³n prÃ¡ctica de consideraciones morales](https://en.wikipedia.org/wiki/Applied_ethics). Es el proceso de investigar activamente cuestiones Ã©ticas en el contexto de _acciones, productos y procesos del mundo real_,  y tomar medidas correctivas para que estos permanezcan alineados con nuestros valores Ã©ticos definidos.

**La cultura Ã©tica** consiste en [_poner en prÃ¡ctica la Ã©tica aplicada_](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantizar que nuestros principios y prÃ¡cticas Ã©ticos se adopten de manera consistente y escalable en toda la organizaciÃ³n. Las culturas Ã©ticas exitosas definen principios Ã©ticos en toda la organizaciÃ³n, brindan incentivos significativos para el cumplimiento y refuerzan las normas Ã©ticas al alentar y amplificar los comportamientos deseados en todos los niveles de la organizaciÃ³n.

## Conceptos de Ã©tica

En esta secciÃ³n, discutiremos conceptos como **valores compartidos** (principios) y **desafÃ­os Ã©ticos** (problemas) para la Ã©tica de los datos, y exploraremos **estudios de casos**que lo ayudarÃ¡n a comprender estos conceptos en contextos del mundo real.

### 1. Principios Ã©ticos

Cada estrategia de Ã©tica de datos comienza con la definiciÃ³n de _principios Ã©ticos_ - los "valores compartidos" que describen comportamientos aceptables y guÃ­an acciones de cumplimiento en nuestros proyectos de datos e inteligencia artificial. Puede definirlos a nivel individual o de equipo. Sin embargo, la mayorÃ­a de las organizaciones grandes los describen en una declaraciÃ³n o marco de misiÃ³n Ã©tica de la IA que se define a nivel corporativo y se aplica de manera consistente en todos los equipos.

**Ejemplo:** la declaraciÃ³n de misiÃ³n [Responsible AI](https://www.microsoft.com/en-us/ai/responsible-ai) de Microsoft dice: _"Estamos comprometidos con el avance de la IA impulsada por principios Ã©ticos que ponen a las personas en primer lugar"_ -  identificando seis principios Ã©ticos en el marco a continuaciÃ³n:

![Responsible AI at Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Exploremos brevemente estos principios. _La transparencia_ y _la rendiciÃ³n de cuentas_ on valores fundamentales sobre los que se basan otros principios, asÃ­ que comencemos por ahÃ­:

* [**La rendiciÃ³n de cuentas**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) hace que los profesionales sean responsables de sus operaciones de datos e inteligencia artificial, y del cumplimiento de estos principios Ã©ticos.
* [**La transparencia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6)  garantiza que los datos y las acciones de la IA sean comprensibles (interpretables) para los usuarios, explicando el quÃ© y el por quÃ© detrÃ¡s de las decisiones.
* [**Equidad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - se centra en garantizar que la IA trate a todas las personas de manera justa, abordando cualquier sesgo sociotÃ©cnico sistÃ©mico o implÃ­cito en los datos y sistemas.
* [**Fiabilidad y seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - garantiza que la IA se comporte de forma coherente con los valores definidos, minimizando posibles daÃ±os o consecuencias no deseadas.
* [**Privacidad y seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - se trata de comprender el linaje de datos y brindar privacidad de datos y protecciones relacionadas a los usuarios.
* [**InclusiÃ³n**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - se trata de diseÃ±ar soluciones de IA con intenciÃ³n, adaptÃ¡ndolas para satisfacer una amplia gama de necesidades y capacidades humanas.

> ðŸš¨ Piense en cuÃ¡l podrÃ­a ser su declaraciÃ³n de misiÃ³n de Ã©tica de datos. Explore los marcos Ã©ticos de IA de otras organizaciones: aquÃ­ hay ejemplos de [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) y [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Â¿QuÃ© valores compartidos tienen en comÃºn? Â¿CÃ³mo se relacionan estos principios con el producto o la industria de IA en la que operan?

### 2. DesafÃ­os Ã©ticos

Una vez que hayamos definido los principios Ã©ticos, el siguiente paso es evaluar nuestros datos y acciones de IA para ver si se alinean con esos valores compartidos. Piense en sus acciones en dos categorÃ­as: recopilaciÃ³n de datos y diseÃ±o de algoritmos. 

Con la recopilaciÃ³n de datos, las acciones probablemente involucrarÃ¡n **datos personales** o informaciÃ³n de identificaciÃ³n personal (PII) de personas vivas identificables. Esto incluye [diversos elementos de datos no personales](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que identifican colectivamente a un individuo. Los desafÃ­os Ã©ticos pueden estar relacionados con la privacidad de los datos , la propiedad de los datos y temas relacionados como el consentimiento informado y los derechos de propiedad intelectual de los usuarios.

Con el diseÃ±o de algoritmos, las acciones implicarÃ¡n recopilar y seleccionar **conjuntos de datos**, uego usarlos para entrenar e implementar **modelos de datos** que predigan resultados o automaticen decisiones en contextos del mundo real. Los desafÃ­os Ã©ticos pueden surgir del sesgo del conjunto de datos , problemas de calidad de los datos , injusticia y tergiversaciÃ³n en los algoritmos, incluidos algunos problemas que son de naturaleza sistÃ©mica.

En ambos casos, los desafÃ­os Ã©ticos resaltan Ã¡reas donde nuestras acciones pueden entrar en conflicto con nuestros valores compartidos. Para detectar, mitigar, minimizar o eliminar estas preocupaciones, debemos hacer preguntas morales de "sÃ­ o no" relacionadas con nuestras acciones y luego tomar las acciones correctivas necesarias. Echemos un vistazo a algunos desafÃ­os Ã©ticos y las cuestiones morales que plantean:


#### 2.1 Propiedad de los datos

La recopilaciÃ³n de datos a menudo implica datos personales que pueden identificar a los interesados. [La propiedad de los datos](https://permission.io/blog/data-ownership) se refiere al control y [_los derechos del usuario_](https://permission.io/blog/data-ownership) relacionados con la creaciÃ³n, el procesamiento y la difusiÃ³n de datos.

Las preguntas morales que debemos plantearnos son: 
 * Â¿A quiÃ©n pertenecen los datos? (usuario u organizaciÃ³n)
 * Â¿QuÃ© derechos tienen los interesados? (ej: acceso, borrado, portabilidad)
 * Â¿QuÃ© derechos tienen las organizaciones? (por ejemplo: rectificar reseÃ±as de usuarios maliciosos)

#### 2.2 Consentimiento informado

[El consentimiento informado](https://legaldictionary.net/informed-consent/) define el acto por el cual los usuarios aceptan una acciÃ³n (como la recopilaciÃ³n de datos) con una comprensiÃ³n completa de los hechos relevantes, incluido el propÃ³sito, los riesgos potenciales y las alternativas.

Las preguntas para explorar aquÃ­ son:
 * Â¿El usuario (titular de los datos) dio permiso para la captura y el uso de datos?
 * Â¿El usuario entendiÃ³ el propÃ³sito para el cual se capturaron esos datos?
 * Â¿El usuario entendiÃ³ los riesgos potenciales de su participaciÃ³n?

#### 2.3 Propiedad intelectual

[La propiedad intelectual](https://en.wikipedia.org/wiki/Intellectual_property) se refiere a creaciones intangibles resultantes de la iniciativa humana, que pueden tener valor econÃ³mico para individuos o empresas.

Las preguntas para explorar aquÃ­ son:
 * Â¿Los datos recopilados tenÃ­an valor econÃ³mico para un usuario o empresa?
 * Â¿El usuario tiene propiedad intelectual aquÃ­?
 * Â¿La **organizaciÃ³n** tiene propiedad intelectual aquÃ­?
 * Si estos derechos existen, Â¿cÃ³mo los protegemos?

#### 2.4 Privacidad de datos

[La privacidad de los datos](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) o la privacidad de la informaciÃ³n se refiere a la preservaciÃ³n de la privacidad del usuario y la protecciÃ³n de la identidad del usuario con respecto a la informaciÃ³n de identificaciÃ³n personal.

Las preguntas para explorar aquÃ­ son:
 * Â¿EstÃ¡n los datos (personales) de los usuarios protegidos contra ataques y filtraciones?
 * Â¿Los datos de los usuarios son accesibles solo para usuarios y contextos autorizados?
 * Â¿Se preserva el anonimato de los usuarios cuando se comparten o difunden datos?
 * Â¿Se puede desidentificar a un usuario de conjuntos de datos anonimizados?


#### 2.5 Derecho al olvido
El [Derecho al Olvido](https://en.wikipedia.org/wiki/Right_to_be_forgotten) or [Derecho de SupresiÃ³n](https://www.gdpreu.org/right-to-be-forgotten/)proporciona protecciÃ³n adicional de datos personales a los usuarios. EspecÃ­ficamente, otorga a los usuarios el derecho de solicitar la eliminaciÃ³n o eliminaciÃ³n de datos personales de bÃºsquedas en Internet y otras ubicaciones, bajo circunstancias especÃ­ficas , permitiÃ©ndoles un nuevo comienzo en lÃ­nea sin que se les apliquen acciones pasadas.

Las preguntas para explorar aquÃ­ son:
 * Â¿El sistema permite a los interesados â€‹â€‹solicitar la supresiÃ³n?
 * Â¿La retirada del consentimiento del usuario deberÃ­a provocar un borrado automÃ¡tico?
 * Â¿Se recopilaron datos sin consentimiento o por medios ilegales?
 * Â¿Cumplimos con las regulaciones gubernamentales sobre privacidad de datos?


#### 2.6 Sesgo del conjunto de datos

El sesgo de conjunto de datos o [recopilaciÃ³n](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) consiste en seleccionar un subconjunto de datos no representativo para el desarrollo de algoritmos, creando una posible injusticia en los resultados para diversos grupos. Los tipos de sesgo incluyen sesgo de selecciÃ³n o muestreo, sesgo de voluntario y sesgo de instrumento.

Las preguntas para explorar aquÃ­ son:
 * Â¿Contratamos a un conjunto representativo de interesados?
 * Â¿Probamos nuestro conjunto de datos recopilados o seleccionados para detectar diversos sesgos?
 * Â¿Podemos mitigar o eliminar cualquier sesgo descubierto?

#### 2.7 Calidad de los datos

[Data Quality](https://lakefs.io/data-quality-testing/) analiza la validez del conjunto de datos seleccionado utilizado para desarrollar nuestros algoritmos, verificando si las caracterÃ­sticas y los registros cumplen con los requisitos del nivel de precisiÃ³n y coherencia necesarios para nuestro propÃ³sito de IA.

Las preguntas para explorar aquÃ­ son:
 * Â¿Captamos caracterÃ­sticas vÃ¡lidas para nuestro caso de uso?
 * Â¿Se capturaron datos de manera consistente en diversas fuentes de datos?
 * Â¿EstÃ¡ completo el conjunto de datos para diversas condiciones o escenarios?
 * Â¿Se captura la informaciÃ³n con precisiÃ³n para reflejar la realidad?

#### 2.8 Equidad del algoritmo

[La equidad del algoritmo ](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) verifica si el diseÃ±o del algoritmo discrimina sistemÃ¡ticamente a subgrupos especÃ­ficos de interesados, lo que genera [daÃ±os potenciales](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) en la asignaciÃ³n (donde se niegan o retienen recursos de ese grupo) y la calidad del servicio (donde la IA no es tan precisa para algunos subgrupos como lo es). es para otros.

Las preguntas para explorar aquÃ­ son:
 * Â¿Evaluamos la precisiÃ³n del modelo para diversos subgrupos y condiciones?
 * Â¿Examinamos el sistema en busca de daÃ±os potenciales (por ejemplo, estereotipos)?
 * Â¿Podemos revisar los datos o volver a entrenar los modelos para mitigar los daÃ±os identificados?

Explore recursos como [las listas de verificaciÃ³n de equidad de IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para obtener mÃ¡s informaciÃ³n.

#### 2.9 TergiversaciÃ³n

[La tergiversaciÃ³n de datos](https://www.sciencedirect.com/topics/computer-science/misrepresentation) consiste en preguntar si estamos comunicando conocimientos a partir de datos informados honestamente de manera engaÃ±osa para respaldar una narrativa deseada.

Las preguntas para explorar aquÃ­ son:
 * Â¿Estamos reportando datos incompletos o inexactos?
 * Â¿Estamos visualizando los datos de una manera que genera conclusiones engaÃ±osas?
 * Â¿Estamos utilizando tÃ©cnicas estadÃ­sticas selectivas para manipular los resultados?
 * Â¿Existen explicaciones alternativas que puedan ofrecer una conclusiÃ³n diferente?

#### 2.10 Libre ElecciÃ³n

La [ilusiÃ³n de la libre elecciÃ³n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice)  ocurre cuando las "arquitecturas de elecciÃ³n" del sistema utilizan algoritmos de toma de decisiones para empujar a las personas a tomar un resultado preferido mientras parecen darles opciones y control. Estos [patrones oscuros](https://www.darkpatterns.org/) pueden causar daÃ±os sociales y econÃ³micos a los usuarios. Debido a que las decisiones de los usuarios afectan los perfiles de comportamiento, estas acciones potencialmente impulsan decisiones futuras que pueden amplificar o extender el impacto de estos daÃ±os.

Las preguntas para explorar aquÃ­ son:
 * Â¿EntendiÃ³ el usuario las implicaciones de tomar esa decisiÃ³n?
 * Â¿El usuario conocÃ­a las opciones (alternativas) y los pros y los contras de cada una?
 * Â¿Puede el usuario revertir una elecciÃ³n automatizada o influenciada mÃ¡s adelante?

### 3. Estudios de caso

To put these ethical challenges in real-world contexts, it helps to look at case studies that highlight the potential harms and consequences to individuals and society, when such ethics violations are overlooked. 

Here are a few examples:

| Ethics Challenge | Case Study  | 
|--- |--- |
| **Consentimiento informado** | 1972 - [Estudio de sÃ­filis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) -  A los hombres afroamericanos que participaron en el estudio se les prometiÃ³ atenciÃ³n mÃ©dica gratuita, pero los investigadores los engaÃ±aron y no informaron a los sujetos sobre su diagnÃ³stico ni sobre la disponibilidad del tratamiento. Muchos sujetos murieron y sus parejas o niÃ±os resultaron afectados; el estudio durÃ³ 40 aÃ±os. | 
| **Privacidad de datos** |  2007 - El [premio de datos de Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) proporcionÃ³ a los investigadores 10 millones de clasificaciones de pelÃ­culas anÃ³nimas de 50.000 clientes para ayudar a mejorar los algoritmos de recomendaciÃ³n. Sin embargo, los investigadores pudieron correlacionar datos anÃ³nimos con datos de identificaciÃ³n personal en conjuntos de datos externos (por ejemplo, comentarios de IMDb), "desanonimizando" efectivamente a algunos suscriptores de Netflix.|
| **Sesgo de colecciÃ³n**  | 2013 -  la ciudad de Boston [desarrollÃ³ Street Bump](https://www.boston.gov/transportation/street-bump), una aplicaciÃ³n que permitÃ­a a los ciudadanos informar sobre baches, brindando a la ciudad mejores datos sobre las carreteras para encontrar y solucionar problemas. Sin embargo, [las personas de los grupos de ingresos mÃ¡s bajos tenÃ­an menos acceso a automÃ³viles y telÃ©fonos](https://hbr.org/2013/04/the-hidden-biases-in-big-data), lo que hacÃ­a que sus problemas viales fueran invisibles en esta aplicaciÃ³n. Los desarrolladores trabajaron con acadÃ©micos para abordar cuestiones de equidad en materia de acceso equitativo y brechas digitales. |
| **Equidad algorÃ­tmica**  | 2018 - The MIT [Gender Shades Study](http://gendershades.org/overview.html) evaluated the accuracy of gender classification AI products, exposing gaps in accuracy for women and persons of color. A [2019 Apple Card](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) seemed to offer less credit to women than men. Both illustrated issues in algorithmic bias leading to socio-economic harms.|
| **TergiversaciÃ³n de datos** | 2020 - The [Georgia Department of Public Health released COVID-19 charts](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) that appeared to mislead citizens about trends in confirmed cases with non-chronological ordering on the x-axis. This illustrates misrepresentation through visualization tricks. |
| **IlusiÃ³n de libre elecciÃ³n** | 2020 - Learning app [ABCmouse paid $10M to settle an FTC complaint](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) where parents were trapped into paying for subscriptions they couldn't cancel. This illustrates dark patterns in choice architectures, where users were nudged towards potentially harmful choices. |
| **Privacidad de datos y derechos del usuario** | 2021 - Facebook [Data Breach](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) exposed data from 530M users, resulting in a $5B settlement to the FTC. It however refused to notify users of the breach violating user rights around data transparency and access. |

Want to explore more case studies? Check out these resources:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - ethics dilemmas across diverse industries. 
* [Data Science Ethics course](https://www.coursera.org/learn/data-science-ethics#syllabus) - landmark case studies explored.
* [Where things have gone wrong](https://deon.drivendata.org/examples/) - deon checklist with examples

> ðŸš¨ Think about the case studies you've seen - have you experienced, or been affected by, a similar ethical challenge in your life? Can you think of at least one other case study that illustrates one of the ethical challenges we've discussed in this section?

## Applied Ethics

We've talked about ethics concepts, challenges ,and case studies in real-world contexts. But how do we get started _applying_ ethical principles and practices in our projects? And how do we _operationalize_ these practices for better governance? Let's explore some real-world solutions: 

### 1. Professional Codes

Professional Codes offer one option for organizations to "incentivize" members to support their ethical principles and mission statement. Codes are _moral guidelines_ for professional behavior, helping employees or members make decisions that align with their organization's principles. They are only as good as the voluntary compliance from members; however, many organizations offer additional rewards and penalties to motivate compliance from members.

Examples include:

 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Code of Ethics
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Code of Conduct (created 2013)
 * [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (since 1993)

> ðŸš¨ Do you belong to a professional engineering or data science organization? Explore their site to see if they define a professional code of ethics. What does this say about their ethical principles? How are they "incentivizing" members to follow the code?

### 2. Ethics Checklists

While professional codes define required _ethical behavior_ from practitioners, they [have known limitations](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) in enforcement, particularly in large-scale projects. Instead, many data Science experts [advocate for checklists](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), that can **connect principles to practices** in more deterministic and actionable ways. 

Checklists convert questions into "yes/no" tasks that can be operationalized, allowing them to be tracked as part of standard product release workflows. 

Examples include:
 * [Deon](https://deon.drivendata.org/) - a general-purpose data ethics checklist created from [industry recommendations](https://deon.drivendata.org/#checklist-citations) with a command-line tool for easy integration.
 * [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - provides general guidance for information handling practices from legal and social exposure perspectives.
 * [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - created by AI practitioners to support the adoption and integration of fairness checks into AI development cycles.
 * [22 questions for ethics in data and AI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - more open-ended framework, structured for initial exploration of ethical issues in design, implementation, and organizational, contexts.

### 3. Ethics Regulations

Ethics is about defining shared values and doing the right thing _voluntarily_. **Compliance** is about _following the law_ if and where defined. **Governance** broadly covers all the ways in which organizations operate to enforce ethical principles and comply with established laws.

Today, governance takes two forms within organizations. First, it's about defining **ethical AI** principles and establishing practices to operationalize adoption across all AI-related projects in the organization. Second, it's about complying with all government-mandated **data protection regulations** for regions it operates in.

Examples of data protection and privacy regulations:

 * `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - regulates _federal govt._ collection, use ,and disclosure of personal information.
 * `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protects personal health data.
 * `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protects data privacy of children under 13.
 * `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - provides user rights, data protection ,and privacy.
 * `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) gives consumers more _rights_ over their (personal) data.
 * `2021`, China's [Personal Information Protection Law](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) just passed, creating one of the strongest online data privacy regulations worldwide.

> ðŸš¨ The European Union defined GDPR (General Data Protection Regulation) remains one of the most influential data privacy regulations today. Did you know it also defines [8 user rights](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) to protect citizens' digital privacy and personal data? Learn about what these are, and why they matter.


### 4. Ethics Culture

Note that there remains an intangible gap between _compliance_ (doing enough to meet "the letter of the law") and addressing [systemic issues](https://www.coursera.org/learn/data-science-ethics/home/week/4) (like ossification, information asymmetry, and distributional unfairness) that can speed up the weaponization of AI. 

The latter requires [collaborative approaches to defining ethics cultures](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) that build emotional connections and consistent shared values _across organizations_ in the industry. This calls for more [formalized data ethics cultures](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) in organizations - allowing _anyone_ to [pull the Andon cord](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (to raise ethics concerns early in the process) and making _ethical assessments_ (e.g., in hiring) a core criteria team formation in AI projects.

---
## [Post-lecture quiz](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/3) ðŸŽ¯
## Review & Self Study 

Courses and books help with understanding core ethics concepts and challenges, while case studies and tools help with applied ethics practices in real-world contexts. Here are a few resources to start with.

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - lesson on Fairness, from Microsoft.
* [Principles of Responsible AI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - free learning path from Microsoft Learn.
* [Ethics and Data Science](https://resources.oreilly.com/examples/0636920203964) - O'Reilly EBook (M. Loukides, H. Mason et. al)
* [Data Science Ethics](https://www.coursera.org/learn/data-science-ethics#syllabus) - online course from the University of Michigan.
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - case studies from the University of Texas.

# Assignment 

[Write A Data Ethics Case Study](assignment.md)
